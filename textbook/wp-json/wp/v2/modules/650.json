{"id":650,"date":"2020-06-28T23:46:29","date_gmt":"2020-06-28T22:46:29","guid":{"rendered":"https:\/\/temp.susodigital.com\/textbook\/?post_type=module&#038;p=650"},"modified":"2020-08-24T09:49:34","modified_gmt":"2020-08-24T08:49:34","slug":"the-crawling-phase","status":"publish","type":"module","link":"https:\/\/susodigital.com\/textbook\/module\/how-google-works\/the-crawling-phase","title":{"rendered":"The Crawling Phase"},"featured_media":0,"parent":133,"menu_order":8,"template":"","acf":{"module_quiz":"","module_time":"45","module_difficulty":{"value":"hard","label":"Hard"},"module_short_description":"<p>An in-depth look at how Google crawls the Internet.<\/p>\n","module_long_description":"<p>Learn what steps Google takes in finding and discovering new content on the Internet. With the help of Google patents, we take a deep dive into crawl scheduling, Google data centers, crawl budget and crawl behaviors.<\/p>\n","video_file":false,"video_url":"","content":"<p>Crawling is the process by which search engines discover new and updated pages to their index. In this section, we\u2019ll walk you through how Google\u2019s bot (aptly called Googlebot) crawls the web.<\/p>\n<p>Googlebot uses an algorithmic process involving computer programs which help determine which sites it should crawl, how often a site (or page) should be crawled, as well as how many pages to fetch from a site.<\/p>\n<p>This process begins with a list of URLs that is generated from a previous crawl process and is augmented with the Sitemap data that is provided by the webmaster.<\/p>\n<p>By visiting each website, Googlebot detects and keeps track of any links discovered on each page to crawl at a later stage. Any new discoveries (new sites, updated sites, dead links) are logged and updated in the Google index.<\/p>\n<img loading=\"lazy\" decoding=\"async\" class=\"alignnone size-full wp-image-651\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/Googlebot.jpg\" alt=\"Googlebot\" width=\"1260\" height=\"987\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/Googlebot.jpg 1260w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/Googlebot-51x40.jpg 51w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/Googlebot-102x80.jpg 102w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/Googlebot-38x30.jpg 38w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/Googlebot-77x60.jpg 77w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/Googlebot-31x24.jpg 31w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/Googlebot-61x48.jpg 61w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/Googlebot-23x18.jpg 23w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/Googlebot-46x36.jpg 46w\" sizes=\"(max-width: 1260px) 100vw, 1260px\" \/>\n<p><strong>How Does Google Know Which Pages Not to Crawl?<\/strong><\/p>\n<div class=\"list-styles list-styles__circle list-styles__circle--primary-color\"><ul>\n<li>Pages that are blocked in the robots.txt file will not be crawled, but still might be indexed if they are linked to by another page. Google is able to infer the content of a page by following the link that is pointing to it, and index the page without actually parsing its contents.<\/li>\n<li>Google cannot crawl any pages that are not accessible by an anonymous user &#8211; this includes any login or other authorisation protection.<\/li>\n<li>Pages that have already been crawled and are considered duplicates of another page are crawled less frequently.<\/li>\n<\/ul>\n<\/div>\n<h2>Scheduling Crawls<\/h2>\n<p>The World Wide Web is constantly growing with millions upon millions of new web pages being created and published. It\u2019s important to remember that search engine crawlers have a capacity in terms of how many pages they can crawl, and how often. Search engines need an efficient and effective way to determine how and when these pages should be crawled.<\/p>\n<p>In 2010 and 2011, Google <a href=\"https:\/\/patentimages.storage.googleapis.com\/df\/fe\/8b\/8a54665319f4bc\/US7725452.pdf\" target=\"_blank\" rel=\"nofollow noopener\">patented<\/a> a systematic method for automatically selecting and scheduling which documents should be crawled based on certain criteria without putting a strain on its servers. This is done by assessing, logging and prioritising which URLs should be crawled, assigning crawl period intervals and creating a schedule for when the bots should crawl them.<\/p>\n<p>We already know that links are at the heart of Google\u2019s crawling process, and that\u2019s exactly the case with the patented system, which discovers links in the following ways:<\/p>\n<ol>\n<li>Direct submissions of URLs i.e. pages that are submitted by webmasters<\/li>\n<li>Crawling of URLs i.e. looking at the outgoing links on crawled pages<\/li>\n<li>Submissions of content containing links from third parties i.e. RSS feeds<\/li>\n<\/ol>\n<p>In this section, we\u2019ll walk you through how Google deals with the pages once they\u2019ve been discovered.<\/p>\n<h3>Crawling Layers<\/h3>\n<p>Google stores document identifiers in a data structure that is composed of three separate layers. Google then assigns a crawl score to each URL. This is based on factors such as how frequently the pages are updated, what their PageRank score is and so on. This determines which layer the document should be assigned to.<\/p>\n<p>Web pages that are assigned a high crawl score are passed onto the next stage (i.e. upgraded from base layer to daily crawl layer). Whereas URLs with a low crawl score are not passed onto the next stage for the next crawling period.<\/p>\n<img loading=\"lazy\" decoding=\"async\" class=\"alignnone size-full wp-image-654\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/managing-items-in-crawl-schedule.jpg\" alt=\"Managing items in crawl schedule\" width=\"1481\" height=\"1159\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/managing-items-in-crawl-schedule.jpg 1481w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/managing-items-in-crawl-schedule-51x40.jpg 51w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/managing-items-in-crawl-schedule-102x80.jpg 102w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/managing-items-in-crawl-schedule-38x30.jpg 38w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/managing-items-in-crawl-schedule-77x60.jpg 77w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/managing-items-in-crawl-schedule-31x24.jpg 31w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/managing-items-in-crawl-schedule-61x48.jpg 61w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/managing-items-in-crawl-schedule-23x18.jpg 23w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/managing-items-in-crawl-schedule-46x36.jpg 46w\" sizes=\"(max-width: 1481px) 100vw, 1481px\" \/>\n<h3>Base Layer<\/h3>\n<p>The base layer of this data structure is split into a sequence of segments which includes a number of URLs that represent a percentage of the addressable URLs spanning the entire Internet.<\/p>\n<p>A controller (aka the URL Scheduler) then selects one of these segments (referred to as the <strong>active segment<\/strong>) to be crawled periodically (i.e. daily) in a round robin manner until all segments have been visited by the robots programs.<\/p>\n<div class=\"tip d-flex align-items-center bg-suso-white\"><svg class=\"tip__icon d-block text-dark-blue line-height-1-5\" enable-background=\"new 0 0 29 37\" viewBox=\"0 0 29 37\" xmlns=\"http:\/\/www.w3.org\/2000\/svg\"><g fill=\"#18217e\" opacity=\".5\"><path d=\"m28.6 26.2-2-3.2c-.2-.3-.3-.7-.3-1l-.7-10.6c-.4-5.7-4.8-10.2-10.5-10.7-6.1-.3-11.3 4.3-11.7 10.4l-.7 10.9c0 .4-.1.7-.3 1l-2 3.1c-.4.7-.5 1.5-.1 2.3.4.7 1.1 1.2 1.9 1.2h24.5c.8 0 1.6-.4 1.9-1.1.4-.8.4-1.6 0-2.3z\"\/><path d=\"m17 31.8c.5 0 1 .3 1.3.7.3.5.3 1 0 1.5-.8 1.4-2.3 2.2-3.8 2.2s-3-.8-3.8-2.2c-.3-.5-.3-1 0-1.5s.7-.7 1.3-.7z\"\/><\/g><\/svg><div class=\"entry-content\">Active segment &#8211; the segment of URLs that is selected by the controller to be crawled.<\/div><\/div>\n<h4>Daily Crawl Layer<\/h4>\n<p>The daily crawl layer consists of a smaller group of URLs that either have a higher crawl score, crawl frequency or both. This layer also includes high priority URLs that are discovered by the crawler during the active segment.<\/p>\n<h4>Real Time Layer<\/h4>\n<p>The real time layer contains an even smaller group of pages which are assigned even higher crawl scores or crawl frequencies. These URLs are crawled multiple times during a given period i.e. multiple times a day. This layer may also contain high priority URLs that have been newly discovered and need to be crawled as soon as possible.<\/p>\n<h3>The Logs<\/h3>\n<p>When a URL is discovered by the crawler, it may collect various information about the page and store them in various logs &#8211; these can be thought of as the \u201cclerks\u201d who keep a record of the URL.<\/p>\n<h4>Link Logs<\/h4>\n<p>The link log stores a list of outbound links of the crawled page. This information is passed onto content filters which check for duplicate content on the page, duplicate file structures as well as the anchor text used within those links.<\/p>\n<h4>History Logs<\/h4>\n<p>As you may have guessed, the history logs contain a record of how frequently the content that is associated with the URL(s) is changing, this is referred to as the <strong>URL change frequency<\/strong>.<\/p>\n<div class=\"tip d-flex align-items-center bg-suso-white\"><svg class=\"tip__icon d-block text-dark-blue line-height-1-5\" enable-background=\"new 0 0 29 37\" viewBox=\"0 0 29 37\" xmlns=\"http:\/\/www.w3.org\/2000\/svg\"><g fill=\"#18217e\" opacity=\".5\"><path d=\"m28.6 26.2-2-3.2c-.2-.3-.3-.7-.3-1l-.7-10.6c-.4-5.7-4.8-10.2-10.5-10.7-6.1-.3-11.3 4.3-11.7 10.4l-.7 10.9c0 .4-.1.7-.3 1l-2 3.1c-.4.7-.5 1.5-.1 2.3.4.7 1.1 1.2 1.9 1.2h24.5c.8 0 1.6-.4 1.9-1.1.4-.8.4-1.6 0-2.3z\"\/><path d=\"m17 31.8c.5 0 1 .3 1.3.7.3.5.3 1 0 1.5-.8 1.4-2.3 2.2-3.8 2.2s-3-.8-3.8-2.2c-.3-.5-.3-1 0-1.5s.7-.7 1.3-.7z\"\/><\/g><\/svg><div class=\"entry-content\">URL change frequency &#8211; a calculation that determines how frequently a URL has changed as well as a note on the last updated date.<\/div><\/div>\n<p>A change weight is also considered &#8211; this is to distinguish the importance of the change i.e. changing a couple of sentences is not the same as adding an entire paragraph to an article. The history log also contains other identifying information such as:<\/p>\n<div class=\"list-styles list-styles__circle list-styles__circle--primary-color\"><ul>\n<li>URL Fingerprint &#8211; essentially an ID for the URL<\/li>\n<li>Timestamp &#8211; indicated the time that the URL was recorded\/crawled<\/li>\n<li>Crawl Status &#8211; indicates whether the URL was successfully crawled or not<\/li>\n<li>Content Checksum &#8211; a numerical value which refers to the content of the downloaded page, if the download was successful. This is what is used to determine when the content on the page has been changed.<\/li>\n<li>Source ID &#8211; indicates how the URL was discovered by the robot, i.e. via an outbound link or an internal repository of documents<\/li>\n<li>Download Time &#8211; how long it took the robot to download the web page<\/li>\n<li>Error Condition &#8211; records any errors that were encountered when attempting to download the web page<\/li>\n<li>Page Rank &#8211; a score computed for a given URL by looking at the number of outbound links pointing to a URL and the page rank of those referencing URLs<\/li>\n<\/ul>\n<\/div>\n<h3>URL Scheduler<\/h3>\n<p>The URL Scheduler (also referred to as the controller by some texts) is essentially the component that is doing most of the heavy lifting &#8211; it\u2019s the supervisor.<\/p>\n<p>Here are some of the jobs that the URL scheduler does:<\/p>\n<div class=\"list-styles list-styles__circle list-styles__circle--primary-color\"><ul>\n<li>Decides which pages Googlebot visits in a given period (or epoch) and stores this information in the data structure.<\/li>\n<li>Selects a segment from the base layer for crawling.<\/li>\n<li>Determines whether to add or remove URLs from the daily layer and the real-time layer based on information stored in the history logs.<\/li>\n<li>Decides how regularly each page should be visited by Googlebot.<\/li>\n<li>Obtains and analyses the URL change frequency.<\/li>\n<li>Checks the importance of a page when scheduling visits from Googlebot.<\/li>\n<\/ul>\n<\/div>\n<h3>Scoring Functions<\/h3>\n<p>Let\u2019s take a look at some of the scoring functions that Google uses to prioritise which pages should be crawled.<\/p>\n<p>Daily Score &#8211; The daily score determines which URLs should be added to the daily crawl or real-time layers.<\/p>\n<p>Keep Score &#8211; Sometimes, the URL scheduler may \u201cdrop\u201d a URL from the system to make room for new URLs. In order to decide whether or not a URL should be dropped, a Keep Score is assigned.<\/p>\n<p>Crawl Score &#8211; The crawl score, as we already know, determines the importance of the URL during the crawling process. This is computed using factors like:<\/p>\n<div class=\"list-styles list-styles__circle list-styles__circle--primary-color\"><ul>\n<li>The current location of the URL (active segment, daily layer or real-time layer)<\/li>\n<li>URL page rank<\/li>\n<li>URL crawl history which may be computed as: <em>crawl score = [page rank] ^ 2 * (change frequency) * (time since last crawl)<\/em><\/li>\n<\/ul>\n<\/div>\n<h3>CrawlRank<\/h3>\n<p>You\u2019re probably wondering how any of this impacts your website. Well, having this insight actually answers one pretty common question:<\/p>\n<p>Why isn\u2019t Google crawling (and by extension indexing) some of my pages?<\/p>\n<p>Apart from technical factors (which we\u2019ll tackle later on in the course) that may be preventing some of your pages from being crawled, it\u2019s possible that the bottom line may just be that Google doesn\u2019t see any value in them.<\/p>\n<p>In other words, the CrawlRank (crawl score) for those pages is so low, that Google keeps pushing it down the queue.<\/p>\n<p>Moreover, the fact that <strong>PageRank<\/strong> (which again, we\u2019ll tackle in much more detail later on) plays such a big role in whether Googlebot crawls your web page(s), puts further emphasis on ensuring that you\u2019re creating the best possible content and user experience for your audience.<\/p>\n<div class=\"tip d-flex align-items-center bg-suso-white\"><svg class=\"tip__icon d-block text-dark-blue line-height-1-5\" enable-background=\"new 0 0 29 37\" viewBox=\"0 0 29 37\" xmlns=\"http:\/\/www.w3.org\/2000\/svg\"><g fill=\"#18217e\" opacity=\".5\"><path d=\"m28.6 26.2-2-3.2c-.2-.3-.3-.7-.3-1l-.7-10.6c-.4-5.7-4.8-10.2-10.5-10.7-6.1-.3-11.3 4.3-11.7 10.4l-.7 10.9c0 .4-.1.7-.3 1l-2 3.1c-.4.7-.5 1.5-.1 2.3.4.7 1.1 1.2 1.9 1.2h24.5c.8 0 1.6-.4 1.9-1.1.4-.8.4-1.6 0-2.3z\"\/><path d=\"m17 31.8c.5 0 1 .3 1.3.7.3.5.3 1 0 1.5-.8 1.4-2.3 2.2-3.8 2.2s-3-.8-3.8-2.2c-.3-.5-.3-1 0-1.5s.7-.7 1.3-.7z\"\/><\/g><\/svg><div class=\"entry-content\">PageRank is a relative score of importance and authority by evaluating the quality and quantity of its links.<\/div><\/div>\n<h2>Google Data Centers<\/h2>\n<p>According to Google, a data center is \u201c<em>a facility with many computers that store and process large amounts of information.<\/em>\u201d<\/p>\n<p>All that web crawling, indexing, and searching takes enormous amounts of computing power, not to mention all of the other products and services that Google offers like GMail and Google Cloud etc.<\/p>\n<p>Whatever Google product you\u2019re using, you\u2019re using the data centers\u2019 power to do it.<\/p>\n<p>In fact, Google&#8217;s employees describe their data centers as \u201cthe brains of the Internet\u201d, \u201cthe engine of the Internet\u201d in the video below, and rightly so.<\/p>\n<iframe loading=\"lazy\" title=\"Inside a Google data center\" width=\"500\" height=\"281\" src=\"https:\/\/www.youtube-nocookie.com\/embed\/XZmGGAbHqa0?feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>\n<p>Each data center serves as a single \u201cnode\u201d in a larger network of data centers that is located across the world &#8211; from Ireland and Chile to India and Singapore!<\/p>\n<p>Google uses distributed crawling to retrieve and discover new data via their data centers.<\/p>\n<p>Distributed crawling is the process where crawlers in a given data center crawls the web servers that are topologically close by, and then propagates the crawled pages across the entire network of peer data centers.<\/p>\n<p>For example, the data center in Singapore will crawl web pages from web servers that are close by, and then distribute it to the other data centers that Google has.<\/p>\n<p>The obvious benefit of this is it enables Google to mine the web much more efficiently as each data center only has to focus on crawling the pages that are geographically served close to it.<\/p>\n<p>In a Google <a href=\"https:\/\/patents.google.com\/patent\/US7299219\" target=\"_blank\" rel=\"nofollow noopener\">patent<\/a> that was filed in 2007, we can see that data centers are used for several other jobs during the crawling process. For example, when a web crawler discovers new content, it relays this information to the Data Center.<\/p>\n<p>The Data Center then conducts at least one of the following steps:<\/p>\n<div class=\"list-styles list-styles__square list-styles__square--primary-color\"><ul>\n<li>Storing the new or changed content<\/li>\n<li>Storing delta changes of a page (small changes)<\/li>\n<li>Data mining<\/li>\n<li>Data processing<\/li>\n<li>Application of data to at least one search engine<\/li>\n<li>Intelligent caching<\/li>\n<\/ul>\n<\/div>\n<img loading=\"lazy\" decoding=\"async\" class=\"alignnone size-full wp-image-656\" src=\"\/textbook\/wp-content\/uploads\/2020\/06\/google-patent-figure-7-crawling-phase-1.png\" alt=\"Google Patent\" width=\"2151\" height=\"2625\" \/>\n<p>As seen in the diagram above, data centers consist of <strong>web mining applications<\/strong> and are in communication with a network of web crawlers. When the crawlers discover new information, the data centers produce a changed link stream and a changed content stream (both of which are identifiers used to denote changes between different versions of a web page that has been crawled).<\/p>\n<div class=\"tip d-flex align-items-center bg-suso-white\"><svg class=\"tip__icon d-block text-dark-blue line-height-1-5\" enable-background=\"new 0 0 29 37\" viewBox=\"0 0 29 37\" xmlns=\"http:\/\/www.w3.org\/2000\/svg\"><g fill=\"#18217e\" opacity=\".5\"><path d=\"m28.6 26.2-2-3.2c-.2-.3-.3-.7-.3-1l-.7-10.6c-.4-5.7-4.8-10.2-10.5-10.7-6.1-.3-11.3 4.3-11.7 10.4l-.7 10.9c0 .4-.1.7-.3 1l-2 3.1c-.4.7-.5 1.5-.1 2.3.4.7 1.1 1.2 1.9 1.2h24.5c.8 0 1.6-.4 1.9-1.1.4-.8.4-1.6 0-2.3z\"\/><path d=\"m17 31.8c.5 0 1 .3 1.3.7.3.5.3 1 0 1.5-.8 1.4-2.3 2.2-3.8 2.2s-3-.8-3.8-2.2c-.3-.5-.3-1 0-1.5s.7-.7 1.3-.7z\"\/><\/g><\/svg><div class=\"entry-content\">Web mining applications are programs that extract the contents of the web page that is crawled.<\/div><\/div>\n<p>These are propagated to the web mining apps which then process, download and store the contents of the pages.<\/p>\n<h2>Google Crawling Patents<\/h2>\n<p>We\u2019ve seen a couple of patents that Google have been granted for their crawling system earlier in this module. However, as you\u2019ve probably guessed, those aren\u2019t the only ones. In this section, we\u2019ll walk you through three more patents that may help give an insight into how Google crawls the Internet.<\/p>\n<h3>Anchor Tag Indexing in a Web Crawler System<\/h3>\n<p>Originally filed in 2003 and granted in 2007, this patent, titled Anchor Tag Indexing in a Web Crawler System, potentially answers a few important questions like:<\/p>\n<ol>\n<li>How do search engines use the anchor text in the links pointing to certain pages?<\/li>\n<li>How are permanent and temporary redirects treated differently by search engines?<\/li>\n<\/ol>\n<p>Previously in the module, we showed how Google determines how often a page should be crawled based on several factors and formulas. Another factor that comes into play when it discovers a new URL or document, is the anchor text that is used to link to this page.<\/p>\n<p>The reason why the anchor text is important, is because it helps provide contextual information about the page before Google even looks at the contents. This is especially important and beneficial if the page we\u2019re looking at contains little to no text; the same applies to image and video files.<\/p>\n<h4>Anchor Text Indexing<\/h4>\n<p>The patent introduces the concept of an anchor map (in addition to link logs which you will remember from the Scheduling Crawls section).<\/p>\n<p>The anchor map contains information about the text that is associated with the links including a number of anchor records that indicate the source URL (i.e. the page where the link was discovered), as well as the URLs of the target pages.<\/p>\n<p>These records are ordered by the documents that they target and may also contain annotation information about the anchors.<\/p>\n<p>The annotation information that is stored may be used in conjunction with other information to determine how relevant a page is to different search queries.<\/p>\n<p>For example, the system may encounter a link pointing to a web page containing a chocolate cake recipe with the text \u201cto view the recipe click here\u201d.<\/p>\n<img loading=\"lazy\" decoding=\"async\" class=\"alignnone size-full wp-image-657\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/anchor-text-indexing.jpg\" alt=\"Anchor text indexing\" width=\"1673\" height=\"1209\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/anchor-text-indexing.jpg 1673w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/anchor-text-indexing-1536x1110.jpg 1536w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/anchor-text-indexing-55x40.jpg 55w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/anchor-text-indexing-111x80.jpg 111w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/anchor-text-indexing-42x30.jpg 42w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/anchor-text-indexing-83x60.jpg 83w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/anchor-text-indexing-33x24.jpg 33w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/anchor-text-indexing-66x48.jpg 66w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/anchor-text-indexing-25x18.jpg 25w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/anchor-text-indexing-50x36.jpg 50w\" sizes=\"(max-width: 1673px) 100vw, 1673px\" \/>\n<p>Although the anchor text might be \u201cclick here\u201d, Google may also keep a record of the additional surrounding text \u201cto view the recipe\u201d.<\/p>\n<p>Without this crucial piece of information, it would be incredibly difficult to glean the context and connection between the two linked pages i.e. that the link is pointing to a recipe page.<\/p>\n<p>Anchor maps are also useful for identifying duplicate content, which is something that Google condones as it offers very little value to the user. In these cases, the anchor text pointing to duplicate pages is used to crawl and index the canonical (most important) version of the page.<\/p>\n<h4>Handling Redirects<\/h4>\n<p>The patent also shines some light on how search engines treat links with temporary or permanent redirects.<\/p>\n<p>Instead of following permanent redirects that are found at the URLs that have been requested for crawling, the source and target URLs (that have been redirected) are sent to the content filters which place these links in the link logs where they are passed to the URL managers.<\/p>\n<p>In this context, the URL managers are used to determine when and if these permanently redirected URLs should be crawled by a robot.<\/p>\n<p>For temporary redirects, the robots will follow and obtain information about the page.<\/p>\n<h3>Configuring Web Crawler to Extract Web Page Information<\/h3>\n<p>This patent titled \u2018Configuring web crawler to extract web page information\u2019 which was filed in 2014 and granted two years later, details how Google\u2019s web crawler is configured to extract the information from a web page.<\/p>\n<img loading=\"lazy\" decoding=\"async\" class=\"alignnone size-full wp-image-658\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/google-patent-extract-the-content-from-webpage.png\" alt=\"Google Patent: Extract the content from webpage\" width=\"1345\" height=\"1601\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/google-patent-extract-the-content-from-webpage.png 1345w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/google-patent-extract-the-content-from-webpage-1290x1536.png 1290w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/google-patent-extract-the-content-from-webpage-34x40.png 34w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/google-patent-extract-the-content-from-webpage-67x80.png 67w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/google-patent-extract-the-content-from-webpage-25x30.png 25w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/google-patent-extract-the-content-from-webpage-50x60.png 50w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/google-patent-extract-the-content-from-webpage-20x24.png 20w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/google-patent-extract-the-content-from-webpage-40x48.png 40w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/google-patent-extract-the-content-from-webpage-15x18.png 15w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/google-patent-extract-the-content-from-webpage-30x36.png 30w\" sizes=\"(max-width: 1345px) 100vw, 1345px\" \/>\n<p>The above screenshot which is taken from the patent depicts the process that Google\u2019s program takes in order to extract the information from a web page during the crawling process.<\/p>\n<p>Let\u2019s walk through this process in detail.<\/p>\n<p>110 &#8211; Obtain the web page that is to be configured and crawled.<\/p>\n<p>112 &#8211; A user selection of a node in the webpage is received. This can be indicated by a click, highlight text with a mouse, double clicking, pressing a button or key, tapping, touching or highlighting etc.<\/p>\n<p>114 &#8211; Web crawling configuration options are presented based on the node that the user selected.<\/p>\n<p>An example from the patent is as follows. If the node is made up of text followed by a submission button, the configuration options for web crawling actions related to extract text and a form submission action would be presented.<\/p>\n<p>In other words, the system determines what it is \u201cseeing\u201d and then decides how to extract the information from what it&#8217;s seeing.<\/p>\n<p>There are several different elements that the web crawling configuration may encounter i.e. text, links, images, events, sub pages and form elements.<\/p>\n<p>116 &#8211; At this stage of the process, the configuration option for the crawling process is selected by the user input. For example, for a login page, the user must enter their username and password into a web form. When the web page is crawled by Googlebot, it automatically enters the username and password provided by the user into the textbox.<\/p>\n<p>118 &#8211; At the final stage, the web crawling action defined by the user input is performed and the web page is crawled according to the configuration option settings as indicated by the user when configuring the web page for crawling. For example, the user selected a picture from a product page on an online store and indicated that they wanted to download the image and extract alternative text.<\/p>\n<p>To summarise, this patent helps us see how Google breaks down the various elements on a web page (such as text and images), and determines what action it needs to take in order to extract the desired information so that it can be indexed.<\/p>\n<h3>Duplicate Document Detection in a Web Crawler System<\/h3>\n<p>In some of the previous patents that we\u2019ve encountered, we\u2019ve touched upon the fact that Google\u2019s programs are actively looking out for and keeping a record of pages that contain duplicate content.<\/p>\n<p>Well, it should come as no surprise then, that they\u2019ve also been granted a patent that specifically focuses on detecting duplicate documents during the web crawling process aptly titled \u2018Duplicate document detection in a web crawler system\u2019.<\/p>\n<p>Why is detecting duplicate content so important?<\/p>\n<p>Let\u2019s look at one of the reasons given in the patent itself: \u201c<em>For example, on the back end of a search engine, if duplicate copies of a same document are treated as different documents not related with one another in terms of their content, this would cause the search engine to waste resources, such as disk space, memory, and\/or network bandwidth, in order to process and manage the duplicate documents.<\/em><\/p>\n<p><em>On the front end, retaining duplicate documents would cause the search engine to have to search through large indices and to use more processing power to process queries. Also, a user\u2019s experience may suffer if diverse content that should be included in the search results is crowded out by duplicate documents.<\/em>\u201d<\/p>\n<p>In a nutshell &#8211; Google wants to focus on crawling and indexing unique pages that will be valuable to the end user.<\/p>\n<p>The patent details some of the previous concepts that were highlighted in the earlier patent, but delves deeper into how the content filter works alongside the duplicate content server (also referred to as the Dupserver) to detect duplicate content.<\/p>\n<p>It\u2019s worth noting that the patent defines duplicate content as \u201c<em>documents that have substantially identical content, and in some embodiments wholly identical content, but different document addresses<\/em>\u201d.<\/p>\n<p>It goes onto categorise three scenarios where duplicated content is encountered by the web crawler:<\/p>\n<ol>\n<li>Two pages are considered duplicate documents if they share the same content but have different URLs.<\/li>\n<li>Two temporary redirect pages that share the same target URL, but have different source URLs.<\/li>\n<li>A normal web page and temporary redirect page where the normal web page\u2019s URL is the target URL of the temporary redirect page. Or, if the content on the normal web page is the same as the temporary redirect page.<\/li>\n<\/ol>\n<p>The patent also details the various methods that it may use to detect duplicate content.<\/p>\n<h4>Content Fingerprint Table<\/h4>\n<p>One of these methods is via fingerprint identification, which involves using fingerprints of the content that is found on the pages to compare between other known documents. This information is stored within content fingerprint tables.<\/p>\n<img loading=\"lazy\" decoding=\"async\" class=\"alignnone size-full wp-image-660\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/content-fingerprint-table.png\" alt=\"Content Fingerprint Table\" width=\"1049\" height=\"597\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/content-fingerprint-table.png 1049w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/content-fingerprint-table-70x40.png 70w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/content-fingerprint-table-141x80.png 141w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/content-fingerprint-table-53x30.png 53w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/content-fingerprint-table-105x60.png 105w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/content-fingerprint-table-42x24.png 42w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/content-fingerprint-table-84x48.png 84w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/content-fingerprint-table-32x18.png 32w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/content-fingerprint-table-63x36.png 63w\" sizes=\"(max-width: 1049px) 100vw, 1049px\" \/>\n<p>When receiving a newly crawled page, the content filter will likely first consult the Dupserver to see if a copy of the web page already exists by performing a content fingerprint table lookup. If one exists, then the CFT is updated with an identifier for the newly discovered page.<\/p>\n<h4>URL Fingerprint Table<\/h4>\n<p>A URL Fingerprint Table is also used to determine which page should be treated as the canonical version using what\u2019s aptly called a URL fingerprint table lookup.<\/p>\n<img loading=\"lazy\" decoding=\"async\" class=\"alignnone size-full wp-image-661\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/url-fingerprint-table.png\" alt=\"URL Fingerprint table\" width=\"1039\" height=\"625\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/url-fingerprint-table.png 1039w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/url-fingerprint-table-66x40.png 66w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/url-fingerprint-table-133x80.png 133w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/url-fingerprint-table-50x30.png 50w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/url-fingerprint-table-100x60.png 100w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/url-fingerprint-table-40x24.png 40w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/url-fingerprint-table-80x48.png 80w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/url-fingerprint-table-30x18.png 30w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/url-fingerprint-table-60x36.png 60w\" sizes=\"(max-width: 1039px) 100vw, 1039px\" \/>\n<p>While executing the UFT lookup, the Dupserver determines whether the newly crawled<br \/>\nwebpage is the canonical page with respect to the page&#8217;s target URL.<\/p>\n<p>Once this has been done, the final action that the content filter and Dupserver perform is the URL \u201cclean up\u201d. This involves taking the URL fingerprints of each URL in the newly crawled page that corresponds to a page that has been permanently redirected to another target page, and replacing it with the URL fingerprint of the target page.<\/p>\n<p>When choosing the canonical, it\u2019s important to note that the page with the highest PageRank (for example) may not always be the one that is selected.<\/p>\n<p>Instead, the patent states that there may be cases where \u201c<em>a canonical page of an equivalence class is not necessarily the document that has the highest score (e.g., the highest page rank or other query-independent metric).<\/em>\u201d<\/p>\n<p>The example given in the patent suggests that Google may keep a record of all of the duplicated pages that it finds, and that when it comes across a newly discovered duplicate, it may look at the PageRank (or another independent ranking factor) to see if the new URL has a significantly higher PageRank.<\/p>\n<h2 id=\"CB\">Crawl Budget<\/h2>\n<p>There are several factors that affect the way search engines will treat the content on your website. Crawl budget is one of them. Understanding crawl budget will help you ensure that your website is crawled effectively by Googlebot.<\/p>\n<p>Coined by SEOs to describe the frequency at which Googlebot (or any web crawler for that matter) visits your website. It wasn\u2019t until <a href=\"https:\/\/webmasters.googleblog.com\/2017\/01\/what-crawl-budget-means-for-googlebot.html\" target=\"_blank\" rel=\"nofollow noopener\">September 2017<\/a> that Gary Illyes from Google broke the silence with an official definition of crawl budget. And importantly, what it means for Googlebot.<\/p>\n<p>According to Illyes, Crawl Budget = Crawl Rate Limit + Crawl Demand<\/p>\n<h3>Crawl Rate Limit<\/h3>\n<p>The crawl rate limit is the maximum number of times that Googlebot can make a request to crawl a given site. This way, Googlebot will not overload your website with too many requests. The CRL is determined by your site&#8217;s health. For instance, a fast loading website will have a higher CRL than a website that has slow response times.<\/p>\n<h3>Crawl Demand<\/h3>\n<p>Crawl demand, is how often Google wants to crawl the pages on your site &#8211; this is based on the popularity and staleness of the content. In other words, the aim for Google is ensure that its index is kept as fresh as possible &#8211; so the most popular pages, and pages that haven\u2019t been visited by Googlebot in a while, are likely to be crawled more frequently.<\/p>\n<p>If we take the two together, we can deduce that Crawl Budget can therefore be defined as the \u201cnumber of URLs that Googlebot can and wants to crawl\u201d.<\/p>\n<h3>How Crawl Budget Affects Your Chances of Ranking<\/h3>\n<p>The ultimate question is whether any of this is going to help you rank, right?<\/p>\n<p>The answer is both a yes, and a no.<\/p>\n<p>Google has confirmed that an increased crawl rate does not necessarily lead to ranking better in the SERPs. That\u2019s because there are hundreds of other (arguably more important) signals that Google uses to determine whether a page should rank or not.<\/p>\n<p>So, whilst the crawl budget may not be a ranking signal, it\u2019s still an important aspect of SEO as if Google doesn\u2019t index a page (for whatever reason), it\u2019s not going to rank for anything.<\/p>\n<h3>A Brief History Lesson On Crawl Rate<\/h3>\n<p>Once upon a time, Google kindly allowed you to increase the crawl rate in Google Search Console.<\/p>\n<p>Although this feature is no longer available, you can file a special request to reduce or limit the crawl rate if you notice that your website\u2019s server load times are being impacted by Googlebot hitting your server too hard.<\/p>\n<h2>Understanding Crawl Behaviours<\/h2>\n<p>In this section, we\u2019ll explore some of the ways that search engines may traverse your website and how your site structure and server speed may impact Google\u2019s crawl behaviour.<\/p>\n<h3>Depth First Search Crawling<\/h3>\n<p>Depth First crawling forces web crawlers to explore the depth of your website before returning back up the site hierarchy.<\/p>\n<p>This proves an especially effective method for quickly identifying internal pages that contain valuable content.<\/p>\n<p>On the flip side, this means that core navigational pages will be pushed down in priority. So if your site structure has lots of levels, then DFS search would not be beneficial as the crawler may focus on crawling irrelevant internal pages as opposed to important landing pages that are higher up the hierarchy.<\/p>\n<img loading=\"lazy\" decoding=\"async\" class=\"alignnone size-full wp-image-663\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/first-search-crawling.jpg\" alt=\"Depth First Search Crawling\" width=\"1961\" height=\"1427\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/first-search-crawling.jpg 1961w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/first-search-crawling-1536x1118.jpg 1536w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/first-search-crawling-1920x1397.jpg 1920w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/first-search-crawling-55x40.jpg 55w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/first-search-crawling-110x80.jpg 110w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/first-search-crawling-41x30.jpg 41w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/first-search-crawling-82x60.jpg 82w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/first-search-crawling-33x24.jpg 33w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/first-search-crawling-66x48.jpg 66w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/first-search-crawling-25x18.jpg 25w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/first-search-crawling-49x36.jpg 49w\" sizes=\"(max-width: 1961px) 100vw, 1961px\" \/>\n<h3>Breadth First Search Crawling<\/h3>\n<p>On the contrary, we have breadth first search crawling which traverses your website on a level-by-level basis i.e. it will only move onto the third level once it\u2019s finished crawling the second level.<\/p>\n<p>By traversing multiple categories in your website, BFS has the benefit of discovering more unique pages in a short period of time.<\/p>\n<p>That being said, whilst BFS is good for preserving your site architecture, it can prove to be slow if your category pages take a long time to respond and load.<\/p>\n<img loading=\"lazy\" decoding=\"async\" class=\"alignnone size-full wp-image-664\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/Breadth-First-Search-Crawling-.jpg\" alt=\"Breadth First Search Crawling\" width=\"1961\" height=\"1427\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/Breadth-First-Search-Crawling-.jpg 1961w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/Breadth-First-Search-Crawling--1536x1118.jpg 1536w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/Breadth-First-Search-Crawling--1920x1397.jpg 1920w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/Breadth-First-Search-Crawling--55x40.jpg 55w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/Breadth-First-Search-Crawling--110x80.jpg 110w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/Breadth-First-Search-Crawling--41x30.jpg 41w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/Breadth-First-Search-Crawling--82x60.jpg 82w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/Breadth-First-Search-Crawling--33x24.jpg 33w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/Breadth-First-Search-Crawling--66x48.jpg 66w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/Breadth-First-Search-Crawling--25x18.jpg 25w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/06\/Breadth-First-Search-Crawling--49x36.jpg 49w\" sizes=\"(max-width: 1961px) 100vw, 1961px\" \/>\n<h3>Efficiency Search Crawling<\/h3>\n<p>When it comes to efficiency search crawling, the crawler bases its crawl behaviour on server speed rather than the site structure.<\/p>\n<p>In other words, if your website has been allocated a crawl budget of an hour, the spider will pick pages with the lowest response times first, and crawl those. This allows the web crawler to traverse more websites in a short period of time.<\/p>\n<p>Therefore, it\u2019s important to ensure that your website responds as quickly as possible to allow the crawler to traverse more web pages within its allocated time frame.<\/p>\n<p>Here are some quick pointers on how to improve your server response time:<\/p>\n<ol>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Choose the right host and server<\/span><\/li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Optimise and configure your web server &#8211; enabling a cache, using a content delivery network (CDN) and making sure you use HTTP\/2 makes a huge difference in reducing the response time of your website.<\/span><\/li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Eliminate Bloat &#8211; remove any programs, apps or plugins that you don\u2019t need and may be dragging your response time lower.<\/span><\/li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Optimise Your Existing Resources &#8211; optimisng your images, CSS and JavaScript files will go a long way in reducing response time.<\/span><\/li>\n<\/ol>\n"},"_links":{"self":[{"href":"https:\/\/susodigital.com\/textbook\/wp-json\/wp\/v2\/modules\/650"}],"collection":[{"href":"https:\/\/susodigital.com\/textbook\/wp-json\/wp\/v2\/modules"}],"about":[{"href":"https:\/\/susodigital.com\/textbook\/wp-json\/wp\/v2\/types\/module"}],"version-history":[{"count":0,"href":"https:\/\/susodigital.com\/textbook\/wp-json\/wp\/v2\/modules\/650\/revisions"}],"up":[{"embeddable":true,"href":"https:\/\/susodigital.com\/textbook\/wp-json\/wp\/v2\/modules\/133"}],"wp:attachment":[{"href":"https:\/\/susodigital.com\/textbook\/wp-json\/wp\/v2\/media?parent=650"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}